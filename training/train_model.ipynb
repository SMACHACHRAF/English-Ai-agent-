{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f52b427",
   "metadata": {},
   "source": [
    "# Entraînement du Chatbot d'Anglais avec LLaMA 3.2\n",
    "\n",
    "Ce notebook permet d'entraîner (fine-tuner) un modèle LLaMA 3.2 ou similaire sur le dataset de questions-réponses pour créer un chatbot d'apprentissage de l'anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3706077",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement\n",
    "\n",
    "Commençons par installer les bibliothèques nécessaires et configurer l'accès à Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montage de Google Drive pour accéder aux données\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Définir le chemin vers votre dossier projet sur Drive\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/DEEP\"  # Ajustez selon votre structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "!pip install transformers datasets accelerate peft trl bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c171dc",
   "metadata": {},
   "source": [
    "## 2. Préparation des données\n",
    "\n",
    "Nous allons préparer les données pour l'entraînement en les divisant en ensembles d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Charger les données\n",
    "with open(f\"{PROJECT_DIR}/data/Question_Reponse_DATA_converted.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} QA pairs\")\n",
    "\n",
    "# Analyser les données\n",
    "instruction_lengths = [len(item[\"instruction\"].split()) for item in data]\n",
    "input_lengths = [len(item[\"input\"].split()) for item in data]\n",
    "output_lengths = [len(item[\"output\"].split()) for item in data]\n",
    "\n",
    "print(\"\\nData Statistics:\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Average instruction length: {sum(instruction_lengths) / len(instruction_lengths):.1f} words\")\n",
    "print(f\"Average input length: {sum(input_lengths) / len(input_lengths):.1f} words\")\n",
    "print(f\"Average output length: {sum(output_lengths) / len(output_lengths):.1f} words\")\n",
    "print(f\"Min/Max instruction length: {min(instruction_lengths)}/{max(instruction_lengths)} words\")\n",
    "print(f\"Min/Max input length: {min(input_lengths)}/{max(input_lengths)} words\")\n",
    "print(f\"Min/Max output length: {min(output_lengths)}/{max(output_lengths)} words\")\n",
    "\n",
    "# Diviser les données\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(f\"Split data into {len(train_data)} training samples and {len(val_data)} validation samples\")\n",
    "\n",
    "# Créer le dossier de sortie\n",
    "os.makedirs(f\"{PROJECT_DIR}/data/processed\", exist_ok=True)\n",
    "\n",
    "# Sauvegarder les données\n",
    "with open(f\"{PROJECT_DIR}/data/processed/train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{PROJECT_DIR}/data/processed/val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e3c5c",
   "metadata": {},
   "source": [
    "## 3. Configuration du modèle et du fine-tuning\n",
    "\n",
    "Nous allons utiliser LLaMA 3.2 1B Instruct et la technique LoRA pour un fine-tuning efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ab4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Définir les paramètres\n",
    "model_name = \"meta-llama/Meta-Llama-3.2-1B-Instruct\"  # Vous pouvez changer pour d'autres modèles\n",
    "output_dir = f\"{PROJECT_DIR}/models/english_tutor_llama3_1b\"\n",
    "train_file = f\"{PROJECT_DIR}/data/processed/train.json\"\n",
    "val_file = f\"{PROJECT_DIR}/data/processed/val.json\"\n",
    "\n",
    "# Si vous n'avez pas accès à LLaMA 3.2, vous pouvez utiliser un modèle ouvert comme:\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# ou\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Créer le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9558fc",
   "metadata": {},
   "source": [
    "### Chargement du modèle et du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour la quantification 4 bits avec BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Charger le modèle quantifié\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Afficher des informations sur le modèle\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3584da",
   "metadata": {},
   "source": [
    "### Chargement des données et configuration de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd77861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données d'entraînement et de validation\n",
    "train_dataset = load_dataset(\"json\", data_files=train_file)[\"train\"]\n",
    "val_dataset = load_dataset(\"json\", data_files=val_file)[\"train\"]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Exemple d'un échantillon\n",
    "print(\"\\nExample of a training sample:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# Créer la configuration LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                     # Dimension des matrices de rang faible\n",
    "    lora_alpha=16,           # Paramètre alpha pour LoRA\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules à adapter\n",
    "    lora_dropout=0.05,       # Dropout pour régularisation\n",
    "    bias=\"none\",             # Ne pas adapter les biais\n",
    "    task_type=\"CAUSAL_LM\"    # Type de tâche\n",
    ")\n",
    "\n",
    "# Configurer les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,                   # Nombre d'époques\n",
    "    per_device_train_batch_size=4,        # Taille de lot\n",
    "    gradient_accumulation_steps=4,        # Accumulation de gradients\n",
    "    optim=\"paged_adamw_32bit\",           # Optimiseur\n",
    "    save_steps=50,                        # Fréquence de sauvegarde\n",
    "    logging_steps=10,                     # Fréquence de logging\n",
    "    learning_rate=2e-5,                   # Taux d'apprentissage\n",
    "    weight_decay=0.001,                   # Régularisation\n",
    "    fp16=True,                            # Utiliser FP16\n",
    "    bf16=False,                           # Ne pas utiliser BF16\n",
    "    max_grad_norm=0.3,                    # Clipping de gradient\n",
    "    max_steps=-1,                         # Utiliser num_train_epochs\n",
    "    warmup_ratio=0.03,                    # Ratio de warmup\n",
    "    group_by_length=True,                 # Grouper par longueur\n",
    "    lr_scheduler_type=\"cosine\",           # Scheduler\n",
    "    evaluation_strategy=\"steps\",          # Évaluer par étapes\n",
    "    eval_steps=50,                        # Fréquence d'évaluation\n",
    "    report_to=\"tensorboard\",              # Reporter à Tensorboard\n",
    "    push_to_hub=False,                    # Ne pas pousser sur Hugging Face Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f68eb",
   "metadata": {},
   "source": [
    "### Préparation du format de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601be71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatter les prompts pour le modèle\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    prompts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        if input_text.strip():\n",
    "            prompt = f\"[INST] {instruction}\\n\\nContext: {input_text} [/INST] {output}\"\n",
    "        else:\n",
    "            prompt = f\"[INST] {instruction} [/INST] {output}\"\n",
    "        prompts.append(prompt)\n",
    "        \n",
    "    return {\"text\": prompts}\n",
    "\n",
    "# Afficher un exemple formaté\n",
    "sample = formatting_prompts_func({\"instruction\": [train_dataset[0][\"instruction\"]], \n",
    "                                \"input\": [train_dataset[0][\"input\"]], \n",
    "                                \"output\": [train_dataset[0][\"output\"]]})\n",
    "print(\"Example of formatted prompt:\")\n",
    "print(sample[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28f64d",
   "metadata": {},
   "source": [
    "### Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50cca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le trainer SFT\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Entraîner le modèle\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343db09c",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to {output_dir}\")\n",
    "# Sauvegarder le modèle\n",
    "trainer.save_model()\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706aa75f",
   "metadata": {},
   "source": [
    "## 4. Test du modèle entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "# Créer un pipeline de génération\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Exemples de test\n",
    "test_examples = [\n",
    "    \"How do I introduce myself in English?\",\n",
    "    \"What's the difference between 'been' and 'gone'?\",\n",
    "    \"Can you explain the present perfect tense?\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting model on examples:\")\n",
    "for example in test_examples:\n",
    "    prompt = f\"[INST] {example} [/INST]\"\n",
    "    print(f\"\\nPrompt: {example}\")\n",
    "    result = pipe(prompt)[0][\"generated_text\"]\n",
    "    \n",
    "    # Extraire la réponse (après [/INST])\n",
    "    response = result.split(\"[/INST]\")[-1].strip()\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e29278",
   "metadata": {},
   "source": [
    "## 5. Exportation du modèle pour l'application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compresser le modèle pour faciliter le téléchargement\n",
    "!cd {output_dir} && tar -czvf ../english_tutor_model.tar.gz .\n",
    "\n",
    "print(f\"Model compressed and saved to {PROJECT_DIR}/models/english_tutor_model.tar.gz\")\n",
    "print(\"You can download this file and extract it to use in your Streamlit app.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
